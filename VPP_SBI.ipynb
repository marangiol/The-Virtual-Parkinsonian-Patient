{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4T3uk0tUYHL"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import matplotlib\n",
        "import cmath\n",
        "import sympy as sym\n",
        "from sympy import exp, symbols, lambdify\n",
        "import math\n",
        "from pytictoc import TicToc\n",
        "import numba\n",
        "from numba import jit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import scipy as scp\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import sbi\n",
        "import sbi.inference\n",
        "from sbi.inference import SNPE, prepare_for_sbi ,simulate_for_sbi, DirectPosterior\n",
        "from sbi.inference.base import infer\n",
        "from sbi.analysis import ActiveSubspace, pairplot\n",
        "import sbi.utils as utils\n",
        "from typing import Any, Callable, Dict, List, Optional, Tuple, Union, cast"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# load the data features from simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_features=np.load(\"NEW_data_features.npy\")\n",
        "c_dopa_data_features=np.load(\"c_dopa.npy\")\n",
        "print(f\"The minimum and maximum values of c_dopa are:{np.min(c_dopa_data_features),np.max(c_dopa_data_features)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_matrix = np.vstack((data_features, c_dopa_data_features))\n",
        "sort_indices = np.argsort(combined_matrix[-1, :])\n",
        "sorted_matrix = combined_matrix[:, sort_indices]\n",
        "all_data_features_sorted=sorted_matrix[:-1,:]\n",
        "c_sorted=sorted_matrix[-1,:]\n",
        "#check for the dimension:\n",
        "print(f\"The number of ckk dopa values is {all_data_features_sorted.shape[1]} and the total features are {all_data_features_sorted.shape[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "titles=['mean_ATM','std_ATM', 'median_ATM', 'sum_ATM', 'skw_ATM', 'kurt_ATM',\n",
        "        'max_eigen_val_ATM','trace_ATM', 'std_diag_ATM','CV_ATM','mom1_ATM','mean_over_kurtosis','kurt_diag','norm_ATM',\n",
        "        'harmonic_mean_ATM','entropy_ATM','mean_FC','mean_kurtosis','mean_co_kurtosis','mean_max_co_kurtosis',\n",
        "        'mean_dev_co_kurtosis','mean_covariance','var_cross_correlation','mean_corr_entropy','var_corr_entropy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "w=120\n",
        "mask=np.ones((1,w))/w\n",
        "mask=mask[0,:]\n",
        "x_inference=[]\n",
        "x_inference_plus=[]\n",
        "x_inference_hamm=[]\n",
        "\n",
        "wind=120\n",
        "w_hamm=120\n",
        "theta_inference=ckk_sorted[(wind-1)//2 : -(wind-1)//2] \n",
        "theta_inference_hamm=ckk_sorted[(w_hamm-1)//2 : -(w_hamm-1)//2] \n",
        "theta_inference_2 = theta_inference[(wind-1)//2 : -(wind-1)//2]\n",
        "for i in range(all_data_features_sorted.shape[0]):\n",
        "    convolved_data=np.convolve(all_data_features_sorted[i,:],mask,'valid')\n",
        "    convolved_data_plus=np.convolve(convolved_data,mask,'valid')\n",
        "    x_inference.append(convolved_data) \n",
        "    x_inference_plus.append(convolved_data_plus)\n",
        "    \n",
        "    w = np.hanning(w_hamm) / np.sum(np.hanning(w_hamm))\n",
        "    y=scp.signal.fftconvolve(all_data_features_sorted[i,:], w, mode='valid')\n",
        "    x_inference_hamm.append(y) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_inference_plus=np.array(x_inference_plus)\n",
        "x_inference_plus.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize variables to store models and their parameters\n",
        "intercepts = []\n",
        "coefficients = []\n",
        "x_projected=[]\n",
        "\n",
        "x_inference_plus=np.array(x_inference_plus)\n",
        "for feature_index in range(x_inference_plus.shape[0]):  \n",
        "    y = x_inference_plus[feature_index, :]  \n",
        "    model = LinearRegression()\n",
        "    model.fit(theta_inference_2.reshape(-1, 1), y)\n",
        "    prediction=model.predict(theta_inference_2.reshape(-1, 1))\n",
        "    x_projected.append(prediction)\n",
        "\n",
        "    intercepts.append(model.intercept_)\n",
        "    coefficients.append(model.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_projected=np.array(x_projected)\n",
        "x_projected.shape\n",
        "theta_inference_2=np.array(theta_inference_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%matplotlib widget\n",
        "fig, axs = plt.subplots(6,5, figsize=(25, 30))\n",
        "\n",
        "for i, ax in enumerate(axs.flat[:min(len(all_data_features_sorted), len(titles))]):\n",
        "    ax.plot(theta_inference_2,x_inference_plus[i],'o',markersize=1)      # second convolution   \n",
        "    ax.plot(theta_inference_2, x_projected[i,:], color='red', label='Linear Regression')\n",
        "    ax.set_title(titles[i],fontsize=14)\n",
        "    ax.legend(('data','Linear Regression'))\n",
        "    ax.set_xlabel('$w_{dopa}$',fontsize=14)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "plt.subplots_adjust(wspace=0.2)\n",
        "\n",
        "plt.tight_layout\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bayesian SBI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To make Bayesian SBI, we need three elements:\n",
        "Simulator that takes parameters as input and returns data features as output.\n",
        "Prior, which defines the plausible range of ckk_Dopa based on the background knowledge, to draw random samples for parameters, as the input to simulator.\n",
        "Inference step on observed data, which we train a deep neural density estimator on data features and approximate posterior for the set of observed data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prior over model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#set the model key parameters that we aim to infer \n",
        "prior_min = [0.9]   # min value used for simulations\n",
        "prior_max = [6]   # max value used for simulations\n",
        "prior = utils.torchutils.BoxUniform(low=torch.as_tensor(prior_min), high=torch.as_tensor(prior_max))\n",
        "num_params_prior=len(prior_min)\n",
        "print('number of params :', num_params_prior)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_ff = np.array(x, dtype='float32').T\n",
        "x_torch = torch.as_tensor(x_ff)\n",
        "theta=np.array(theta).reshape(-1,1)    \n",
        "theta_ff = np.array(theta, dtype='float32').T\n",
        "theta_torch = torch.as_tensor(theta_ff).view(-1, 1)\n",
        "print( 'theta shape:',theta_torch.shape,flush=True)\n",
        "print('features shape:', x_torch.shape,flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we run the inferece step. To do so, we run a budget of random simulations; for each the data feature is calculated, and then an algorithm such as SNPE lerans the relationship between the data features and parameters of an approximated posterior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def myinfer_SNPE(prior, theta, x):\n",
        "    inference = SNPE(prior)\n",
        "    _ = inference.append_simulations(theta, x).train()\n",
        "    posterior = inference.build_posterior()\n",
        "    return posterior"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n=np.random.randint(0, 100000000)\n",
        "np.random.seed(n)\n",
        "torch.manual_seed(n)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "idx_features =[0,3,4,5,9,10,13,15,16,17] #example of index of features to take into account\n",
        "\n",
        "posterior = myinfer_SNPE(prior, theta_torch, x_torch[:,idx_features])\n",
        "\n",
        "print (\"-\"*60)\n",
        "print(\"--- Training neural network took: %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import empirical data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import here the features computed from your empirical data\n",
        "statistics_observed_ON=np.load(\"statistics_observed_ON.npy\")\n",
        "statistics_observed_OFF=np.load(\"statistics_observed_OFF.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "statistics_observed_ON=statistics_observed_ON[idx_features,:]\n",
        "statistics_observed_OFF=statistics_observed_OFF[idx_features,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "num_features = statistics_observed_ON.shape[0]\n",
        "num_subjects = statistics_observed_ON.shape[1]\n",
        "\n",
        "num_rows = num_features // 2 + (1 if num_features % 2 else 0)  \n",
        "num_cols = 2 if num_features > 2 else num_features  \n",
        "fig, axs = plt.subplots(num_rows, num_cols, figsize=(8, 3 * num_rows))\n",
        "\n",
        "axs = axs.flatten()\n",
        "# Iterate through each feature\n",
        "for j in range(num_features):\n",
        "    data_ON = statistics_observed_ON[j, :]\n",
        "    data_OFF = statistics_observed_OFF[j, :]\n",
        "    axs[j].plot(data_ON, 'o', label='ON', color='blue')  \n",
        "    axs[j].plot(data_ON, alpha=0.5, color='blue')\n",
        "\n",
        "    axs[j].plot(data_OFF,'*',label='OFF',color='orange')\n",
        "    axs[j].plot(data_OFF,alpha=0.5,color='orange')\n",
        "    axs[j].set_title(titles[j])\n",
        "    axs[j].legend()\n",
        "    axs[j].set_xticks(np.arange(num_subjects))\n",
        "    axs[j].set_xticklabels(np.arange(1, num_subjects + 1))\n",
        "    \n",
        "for k in range(j + 1, len(axs)):\n",
        "    axs[k].axis('off')\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.tight_layout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# training network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_samples=100000\n",
        "num_patients=10\n",
        "start_time = time.time()\n",
        "posterior_samples_OFF = np.zeros((num_patients, num_samples,1))\n",
        "posterior_samples_ON=np.zeros((num_patients,num_samples,1))\n",
        "\n",
        "for i in range(num_patients):\n",
        "    posterior_samples_OFF[i] = posterior.sample((num_samples,), statistics_observed_OFF[idx_features, i]).numpy()\n",
        "    posterior_samples_ON[i] = posterior.sample((num_samples,), statistics_observed_ON[idx_features, i]).numpy()\n",
        "\n",
        "print (\"-\"*60)\n",
        "print(\"--- posterior sampling took: %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%matplotlib widget\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "num_patients = len(posterior_samples_OFF)  # Assuming posterior_samples_OFF is an array with a shape like (num_patients, num_samples, 1)\n",
        "positions = np.arange(num_patients) + 1\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "\n",
        "for i in range(num_patients):\n",
        "    # Plot OFF condition\n",
        "    vplot_off = plt.violinplot(posterior_samples_OFF[i, :, 0], positions=[positions[i] - 0], widths=0.6, showmeans=True, showextrema=True)\n",
        "    for pc in vplot_off['bodies']:\n",
        "        pc.set_facecolor('orange')\n",
        "        pc.set_edgecolor('orange')\n",
        "    plt.setp(vplot_off['cbars'], color='orange')\n",
        "    plt.setp(vplot_off['cmeans'], color='orange')\n",
        "    plt.setp(vplot_off['cmins'], color='orange')\n",
        "    plt.setp(vplot_off['cmaxes'], color='orange')\n",
        "\n",
        "    # Plot ON condition\n",
        "    vplot_on = plt.violinplot(posterior_samples_ON[i, :, 0], positions=[positions[i] + 0], widths=0.6, showmeans=True, showextrema=True)\n",
        "    for pc in vplot_on['bodies']:\n",
        "        pc.set_facecolor('blue')\n",
        "        pc.set_edgecolor('blue')\n",
        "    plt.setp(vplot_on['cbars'], color='blue')\n",
        "    plt.setp(vplot_on['cmeans'], color='blue')\n",
        "    plt.setp(vplot_on['cmins'], color='blue')\n",
        "    plt.setp(vplot_on['cmaxes'], color='blue')\n",
        "\n",
        "# Create invisible lines for the legend\n",
        "plt.plot([], [], color='blue', label='ON')\n",
        "plt.plot([], [], color='orange', label='OFF')\n",
        "\n",
        "plt.ylabel(r\"$\\tilde{w}_{\\mathrm{dopa}}$\", fontsize=14)  \n",
        "plt.xlabel(\"#Patient\", fontsize=10)\n",
        "plt.xticks(positions, np.arange(1, num_patients + 1), fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# statistical analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "b=0.9 #lower bound\n",
        "a=6 #upper value\n",
        "prior_std =np.sqrt((1/12)*(b-a)**2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def shrinkage(prior_std, post_std):\n",
        "    return 1-(post_std / prior_std)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy import stats\n",
        "from scipy.stats import ttest_ind\n",
        "num_patients=10\n",
        "posterior_samples_OFF=np.squeeze(posterior_samples_OFF)\n",
        "posterior_samples_ON=np.squeeze(posterior_samples_ON)\n",
        "wasserstein_distances=np.zeros(num_patients)\n",
        "t_stat=np.zeros(num_patients)\n",
        "p_value=np.zeros(num_patients)\n",
        "p_value_ttest=np.zeros(num_patients)\n",
        "shrink_ON=np.zeros(num_patients)\n",
        "shrink_OFF=np.zeros(num_patients)\n",
        "\n",
        "for i in range(num_patients):\n",
        "    wasserstein_distances[i] = stats.wasserstein_distance(posterior_samples_OFF[i], posterior_samples_ON[i])\n",
        "    _, p_value[i] = stats.ks_2samp(posterior_samples_OFF[i], posterior_samples_ON[i])\n",
        "    _, p_value_ttest[i] = ttest_ind(posterior_samples_OFF[i], posterior_samples_ON[i])\n",
        "\n",
        "    shrink_ON[i]=shrinkage(prior_std,np.std(posterior_samples_ON[i]))    \n",
        "    shrink_OFF[i]=shrinkage(prior_std,np.std(posterior_samples_OFF[i]))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))  # Increased figure size for better clarity\n",
        "patient_indices=([1,2,3,4,5,6,7,8,9,10])\n",
        "# Subplot for Shrinkage values\n",
        "plt.subplot(1, 2, 1)  # 2 rows, 1 column, first subplot\n",
        "plt.scatter(patient_indices, shrink_ON, color='blue', label='ON')\n",
        "plt.plot(patient_indices, shrink_ON, color='blue',alpha=0.2)\n",
        "plt.plot(patient_indices, shrink_OFF, color='orange',alpha=0.2)\n",
        "\n",
        "plt.scatter(patient_indices, shrink_OFF, color='orange', label='OFF')\n",
        "plt.xlabel('#Patient',fontsize=12)\n",
        "plt.title('Posterior Shrinkage',fontsize=12)\n",
        "plt.ylim([0, 1.1])  # Adjusted for logarithmic scale\n",
        "#plt.yscale('log')  # Set y-axis to logarithmic scale\n",
        "plt.legend()\n",
        "\n",
        "# Subplot for Wasserstein distances\n",
        "plt.subplot(1, 2, 2)  # 2 rows, 1 column, second subplot\n",
        "plt.scatter(patient_indices, wasserstein_distances, color='green')\n",
        "plt.plot(patient_indices, wasserstein_distances, color='green',alpha=0.2)\n",
        "plt.xlabel('#Patient',fontsize=12)\n",
        "plt.title('Wasserstein Distance',fontsize=12)\n",
        "plt.ylim(0,1)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()  # Adjust layout to prevent overlap\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
