{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy import stats as spstats\n",
        "from scipy.stats import moment, kurtosis, skew, mode, zscore, pearsonr\n",
        "from scipy.optimize import fsolve, root\n",
        "import dask.array as da\n",
        "from matplotlib import pyplot as plt\n",
        "import sympy as sym\n",
        "from sympy import exp, symbols, lambdify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "memmap_array = np.lib.format.open_memmap('matrix_all.npy', mode='r')\n",
        "memmap_array_time = np.lib.format.open_memmap('t_all.npy', mode='r')\n",
        "matrix_C_XXX=np.load(\"ckk_all.npy\")\n",
        "L6=np.load(\"L6_regions2channels_abs_sum.npy\")\n",
        "\n",
        "# Create a Dask array from the memory-mapped NumPy array\n",
        "# Adjust 'chunks' based on your available system memory and desired chunk size\n",
        "chunks_size = (400, 528, 10000) \n",
        "chunks_size_time = (10000) \n",
        "dask_array = da.from_array(memmap_array, chunks=chunks_size)\n",
        "dask_array_time = da.from_array(memmap_array_time, chunks=chunks_size_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_trans=2000\n",
        "signals_rate_0=(dask_array[:,0:88,time_trans:]).compute()\n",
        "signals_rate=signals_rate_0.T\n",
        "print(f\"Dimension signals_rate (combinations,nodes,time):\", signals_rate_0.shape)\n",
        "V_0=(dask_array[:,88:176,time_trans:]).compute()\n",
        "V=V_0.T\n",
        "print(f\"Dimension V (combinations,nodes,time):\", V_0.shape)\n",
        "t_all=(dask_array_time[time_trans:]).compute()\n",
        "DP_0=(dask_array[:,440:528,time_trans:]).compute()\n",
        "Dp=DP_0.T\n",
        "print(f\"Dimension [Dp] (combinations,nodes,time):\", DP_0.shape)\n",
        "ckk_inh=matrix_C_XXX[0][0]\n",
        "ckk_exc=matrix_C_XXX[0][1]\n",
        "ckk_dopa_min=np.min(matrix_C_XXX[:,2])\n",
        "ckk_dopa_max=np.max(matrix_C_XXX[:,2])\n",
        "print(f\"min_combination:{ckk_inh},{ckk_exc},{ckk_dopa_min}\")\n",
        "print(f\"max_combination:{ckk_inh},{ckk_exc},{ckk_dopa_max}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_DP_over_regions = np.mean(Dp[:,:,:],axis=1) \n",
        "std_DP = np.std(Dp[:,:,:],axis=1, ddof=1)  # ddof=1 for sample standard deviation\n",
        "print(f\"the mean Dp concentration of all network. Shape of vector:\",{mean_DP_over_regions.shape})\n",
        "mean_DP_over_time = np.mean(Dp[:,:,:],axis=0)  \n",
        "std_DP_regions = np.std(Dp[:,:,:],axis=0, ddof=1)  # ddof=1 for sample standard deviation\n",
        "\n",
        "DP=np.mean(mean_DP_over_time,axis=0)\n",
        "std_DP=np.std(mean_DP_over_time,axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# functions to compute avalanches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# avalanches_global_pattern(x, n)\n",
        "# f is a matrix (containing the consecutive series of activation of one patients (by consecutive I mean that all the clean bits have been attached one after the other)\n",
        "# of axb, where a is the number of areas and b is the time. The argument n will specify how many regions to take into account. h will provide y vectors, with y the number\n",
        "# of avalanches, each vector being long n, that is full with logicals where 0 means that that particular regions has never been active in that\n",
        "# particular avalanches, and 1 means the opposite. If a region has been active only in 1 time bin or in more than 1 time bin, it will be 1 in the vector eitherway.\n",
        "\n",
        "def avalanches_global_pattern(x, n):\n",
        "    x = x[:n, :]\n",
        "    activations_bins = np.where(np.any(x, axis=0))[0]\n",
        "    activations_bins = np.concatenate((activations_bins, [5, 6]))  # Adding dummy values at the end\n",
        "    gg = np.diff(activations_bins) != 1\n",
        "    Index_start = np.where(gg == 1)[0]\n",
        "\n",
        "    f = []\n",
        "    h = []\n",
        "    f.append(x[:, activations_bins[0]:activations_bins[Index_start[0]]+1])\n",
        "\n",
        "    for ii in range(len(Index_start) - 1):\n",
        "        f.append(x[:, activations_bins[Index_start[ii]+1]:activations_bins[Index_start[ii + 1]]+1]) \n",
        "\n",
        "    for kk in range(len(f)):\n",
        "        a = np.sum(f[kk], axis=1)\n",
        "        h.append(a)\n",
        "    return f, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# func_transition_matrix\n",
        "# input: binary matrix b representing active region, structured as regions x %times. Out is an asymmetric matrix, called transition matrix, \n",
        "# containing the probability that, if not i is active, node j will be active in the next timestep\n",
        "\n",
        "def func_transition_matrix(b):\n",
        "    num_regions, num_times = b.shape\n",
        "    out = np.zeros((num_regions, num_regions))\n",
        "    \n",
        "    num_activations_per_reg = np.sum(b, axis=1)\n",
        "    \n",
        "    for kk1 in range(num_regions):\n",
        "        g_loop = np.zeros(num_regions)\n",
        "        for kk2 in range(num_times - 1):\n",
        "            if b[kk1, kk2] == 1:\n",
        "                curr_reg = np.repeat(b[kk1, kk2], num_regions)\n",
        "                next_reg = b[:, kk2 + 1]\n",
        "                g = curr_reg == next_reg\n",
        "                g_loop += g\n",
        "        g_loop[g_loop != 0] /= num_activations_per_reg[kk1]\n",
        "        out[kk1, :] = g_loop\n",
        "    \n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute_ATM(b1)\n",
        "# b1 is the binarized matrix.\n",
        "# num_cc is the number of conditions or channels.\n",
        "# output: ATM and number of avalanches (num_avalan) for each channel or condition.\n",
        "\n",
        "def compute_ATM(b1):\n",
        "    num_avalan = np.zeros(b1.shape[2])\n",
        "    ttt_noaval_accumulated = []\n",
        "    for cc in range(b1.shape[2]):\n",
        "        (aval_nobin,h) = avalanches_global_pattern(b1[:,:,cc], b1.shape[0])\n",
        "\n",
        "        num_avalanche = np.zeros(len(aval_nobin))\n",
        "        temp_TMs_nobin = np.zeros((b1.shape[0], b1.shape[0], len(aval_nobin)))\n",
        "\n",
        "        for kk_aval in range(len(aval_nobin)):\n",
        "            num_avalanche[kk_aval] = aval_nobin[kk_aval].shape[1]\n",
        "            tt_nobin = func_transition_matrix(aval_nobin[kk_aval])\n",
        "            temp_TMs_nobin[:, :, kk_aval] = (tt_nobin + tt_nobin.T) / 2\n",
        "\n",
        "        num_avalan[cc] = np.mean(num_avalanche)\n",
        "\n",
        "        ttt_noaval = np.sum(temp_TMs_nobin, axis=2) / np.sum(temp_TMs_nobin != 0, axis=2)\n",
        "        ttt_noaval[np.isnan(ttt_noaval)] = 0\n",
        "        ttt_noaval_accumulated.append(ttt_noaval)\n",
        "        \n",
        "    return num_avalan, np.array(ttt_noaval_accumulated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# compute LL\n",
        "import numpy as np\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def compute_LL(V, L6):\n",
        "    V = np.transpose(V, (2, 1, 0)) #V ~ combinations x regions x time\n",
        "    print(f\"Shape of V (comb,regions,time) ={V.shape}\")\n",
        "    rr1 = V[:, 0:34, :]\n",
        "    rr2 = V[:, 52:86, :]\n",
        "    reduced_V = np.concatenate((rr1, rr2), axis=1)\n",
        "    \n",
        "    # Step 3: Compute regions2channel64_nozscored and regions2channel6_nozscored\n",
        "    regions2channel6_nozscored = np.dot(L6, reduced_V)\n",
        "    \n",
        "    # Step 4: Extract STN time series and reshape\n",
        "    STN_lh_time_series_nozscored = V[:, 42, :].reshape(1, -1, V.shape[2])\n",
        "    STN_rh_time_series_nozscored = V[:, 45, :].reshape(1, -1, V.shape[2])\n",
        "    Final_matrix_6_nozscored = np.concatenate((regions2channel6_nozscored, STN_lh_time_series_nozscored,STN_rh_time_series_nozscored), axis=0)\n",
        "    \n",
        "    # Step 6: Z-score normalization\n",
        "    Final_matrix_6 = np.array([zscore(matrix, axis=1) for matrix in Final_matrix_6_nozscored])\n",
        "    \n",
        "    # Step 7: Transpose matrices\n",
        "    LL_6 = np.transpose(Final_matrix_6, (0, 2, 1))\n",
        "    \n",
        "    return LL_6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_over_regions = np.mean(signals_rate[:,:,:],axis=1) \n",
        "std = np.std(signals_rate[:,:,:],axis=1)  \n",
        "mean_over_time = np.mean(signals_rate[:,:,:],axis=0)  \n",
        "std_regions = np.std(signals_rate[:,:,:],axis=0) \n",
        "\n",
        "rr=np.mean(mean_over_time,axis=0)\n",
        "std=np.std(mean_over_time,axis=0)\n",
        "rr2=np.mean(mean_over_regions,axis=0)\n",
        "std2=np.std(mean_over_regions,axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "meanV_over_regions = np.mean(V[:,:,:],axis=1) \n",
        "stdV = np.std(V[:,:,:],axis=1)  \n",
        "print(f\"the mean V of all network. Shape of vector: {meanV_over_regions.shape} and the min and max value are: {np.min(meanV_over_regions)},{np.max(meanV_over_regions)}\")\n",
        "meanV_over_time = np.mean(V[:,:,:],axis=0)  \n",
        "stdV_regions = np.std(V[:,:,:],axis=0) \n",
        "print(f\"the mean V of each node. Shape of vector: {meanV_over_time.shape} and the min and max value are: {np.min(meanV_over_time)},{np.max(meanV_over_time)}\")\n",
        "\n",
        "VV=np.mean(meanV_over_time,axis=0)\n",
        "stdV=np.std(meanV_over_time,axis=0)\n",
        "print(f\"the mean V over all the nodes for each cc. Shape of vector: {VV.shape} and the min and max value are: {np.min(VV)},{np.max(VV)}\")\n",
        "VV2=np.mean(meanV_over_regions,axis=0)\n",
        "std2V=np.std(meanV_over_regions,axis=0)\n",
        "print(f\"the mean V over all time for each cc. Shape of vector: {VV2.shape} and the min and max value are: {np.min(VV2)},{np.max(VV2)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cc_idx=[352,43,62] # if you want to chose only some combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# plot for rate time series and fast variables plane\n",
        "%matplotlib widget\n",
        "num_comb=len(cc_idx)\n",
        "fig, axes = plt.subplots(nrows=num_comb, ncols=1, figsize=(6, 5),sharex='col')  # Share X-axis across subplots\n",
        "\n",
        "for i, cc in enumerate(cc_idx):\n",
        "    ax = axes[i]\n",
        "    for region in range(signals_rate.shape[1]):\n",
        "        ax.plot(signals_rate[4000:, region, cc],alpha=0.9)\n",
        "    \n",
        "    ax.set_ylabel('r',fontsize=20)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "    ax.spines['bottom'].set_visible(False)\n",
        "    \n",
        "    # Add a legend indicating the specific value\n",
        "    c_dopa = matrix_C_XXX[cc][2]\n",
        "    legend_text = f'$w_{{dopa}}$ = {c_dopa:.2f}'  \n",
        "    handle, = ax.plot([], [], label=legend_text)\n",
        "    \n",
        "    if i == num_comb - 1:\n",
        "        ax.set_xlabel('Time (ms)', fontsize=20)\n",
        "        ax.spines['bottom'].set_visible(True)\n",
        "    else:\n",
        "        ax.tick_params(labelbottom=False)  \n",
        "    \n",
        "    ax.set_xlim(0, 4000)\n",
        "\n",
        "plt.subplots_adjust(hspace=0.01)  # Adjust hspace according to your needs\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold =2\n",
        "binarized_matrix = np.where(signals_rate_zscored > threshold, 1, 0)\n",
        "print(f\"The dimension of binarized_matrix is {binarized_matrix.shape}\")\n",
        "activations = binarized_matrix.sum(axis=0)\n",
        "print(f\"For each combination, compute the number of activations for each region over time, so the dimension of mean_activations is {activations.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lead field matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LL_6 = compute_LL(V, L6)\n",
        "print(f\"LL_6 shape: {LL_6.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 2.3\n",
        "binarized_matrix = np.where(LL_6> threshold, 1, 0)\n",
        "(num_avalan, ttt_noaval)=compute_ATM(binarized_matrix)\n",
        "print(f\"The dimension of binarized_matrix is {binarized_matrix.shape}\")\n",
        "print(f\"for each cc combination we have an ATM, so in this case we have {len(ttt_noaval)} matrices\")\n",
        "print(f\"Example for one combination _ regions x regions x num_avalanches: {ttt_noaval[0].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# compute the data features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#def calculate_summary_statistics_on_ATM(matrixATM):\n",
        "\n",
        "from scipy.linalg import eig, det\n",
        "from numpy.linalg import norm\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.stats import hmean\n",
        "\n",
        "def calculate_summary_statistics_on_ATM(matrixATM):\n",
        "    summary_statistics = []\n",
        "    mean_ATM = np.zeros(matrixATM.shape[0])\n",
        "    std_ATM = np.zeros(matrixATM.shape[0])\n",
        "    median_ATM = np.zeros(matrixATM.shape[0])\n",
        "    sum_ATM = np.zeros(matrixATM.shape[0])\n",
        "    skw_ATM = np.zeros(matrixATM.shape[0])\n",
        "    kurt_ATM = np.zeros(matrixATM.shape[0])\n",
        "    max_eigen_val_ATM = np.zeros(matrixATM.shape[0])\n",
        "    trace_ATM = np.zeros(matrixATM.shape[0])\n",
        "    std_diag_ATM = np.zeros(matrixATM.shape[0])\n",
        "    CV_ATM = np.zeros(matrixATM.shape[0])\n",
        "    mom1_ATM = np.zeros(matrixATM.shape[0])\n",
        "    mean_over_kurtosis= np.zeros(matrixATM.shape[0])\n",
        "    kurt_diag= np.zeros(matrixATM.shape[0])\n",
        "    norm_ATM = np.zeros(matrixATM.shape[0])\n",
        "    harmonic_mean_ATM = np.zeros(matrixATM.shape[0])\n",
        "    entropy_ATM=np.zeros(matrixATM.shape[0])\n",
        "    \n",
        "    for cc in range(matrixATM.shape[0]):\n",
        "        atm = matrixATM[cc, :, :]\n",
        "        #mean_ATM[cc] = np.mean(atm)                     # Compute mean\n",
        "        mean_ATM[cc] = np.log(np.mean(atm))                     # Compute mean\n",
        "        std_ATM[cc] = np.std(atm, ddof=1)               # Compute standard deviation\n",
        "        nonzero_elements = atm[atm != 0]                # Compute median\n",
        "        #median_ATM[cc] = np.median(nonzero_elements)\n",
        "        median_ATM[cc] = np.log(np.median(nonzero_elements))\n",
        "        #sum_ATM[cc] = np.sum(atm)                       # Compute sum\n",
        "        sum_ATM[cc] = np.log(np.sum(atm))                       # Compute sum\n",
        "        harmonic_mean_ATM[cc] = hmean(atm.ravel())\n",
        "        #skw_ATM[cc] = skew(atm.flatten())               # Compute skewness\n",
        "        skw_ATM[cc] = np.exp(skew(atm.flatten()))               # Compute skewness\n",
        "\n",
        "        kurt_ATM[cc] = kurtosis(atm.flatten())          # Compute kurtosis\n",
        "        mean_over_kurtosis[cc]=np.mean(atm)/kurtosis(atm.flatten())\n",
        "        #trace_ATM[cc] = np.trace(atm)                   # Compute trace\n",
        "        trace_ATM[cc] = np.exp(np.trace(atm))                   # Compute trace\n",
        "        diag_elements = np.diag(atm)                    # Extract diagonal elements\n",
        "        std_diag_ATM[cc] = np.std(diag_elements, ddof=1) # Compute standard deviation of diagonal elements\n",
        "        kurt_diag[cc]=kurtosis(diag_elements)                    # Extract diagonal elements\n",
        "        norm_ATM[cc]=np.linalg.norm(atm)\n",
        "        # mom1_ATM[cc]= np.log10(np.mean(atm)/np.std(atm))  \n",
        "        # CV_ATM[cc]= np.log10(np.std(atm)/np.mean(atm))\n",
        "        mom1_ATM[cc]= (np.mean(atm)/np.std(atm))  \n",
        "        CV_ATM[cc]= (np.std(atm)/np.mean(atm))\n",
        "        #CV_ATM[cc]= np.mean(atm)/np.std(atm, ddof=1)\n",
        "        entropy_ATM[cc]=matrix_entropy(atm, bins='auto', norm=True)\n",
        "                \n",
        "        if atm.shape[0] == atm.shape[1]:\n",
        "            eigen_vals_ATM, _ = eig(atm)  \n",
        "            #max_eigen_val_ATM[cc] = np.max(eigen_vals_ATM)\n",
        "            max_eigen_val_ATM[cc] = np.exp(np.max(eigen_vals_ATM))    \n",
        "\n",
        "    summary_statistics.append([\n",
        "            mean_ATM, std_ATM, median_ATM, sum_ATM, skw_ATM, kurt_ATM,\n",
        "            max_eigen_val_ATM,trace_ATM, std_diag_ATM,CV_ATM,mom1_ATM, mean_over_kurtosis,kurt_diag,norm_ATM,\n",
        "            harmonic_mean_ATM,entropy_ATM\n",
        "        ])\n",
        "\n",
        "    return np.array(summary_statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def entropy(p, tol=1e-8):\n",
        "    \"\"\"Calculate the entropy of a probability distribution, ensuring no zero probability values.\"\"\"\n",
        "    p = np.clip(p, tol, 1 - tol)  # Prevent division by zero or log of zero\n",
        "    return -np.sum(p * np.log2(p))\n",
        "\n",
        "def matrix_entropy(A, bins=5, norm=True):\n",
        "    \"\"\"Calculate the entropy of the matrix A using histogram bins.\"\"\"\n",
        "    A = A.flatten() \n",
        "    # Generate histogram\n",
        "    try:\n",
        "        p, bin_edges = np.histogram(A, bins=bins)\n",
        "    except Exception as e:\n",
        "        # Catch all exceptions to understand the issue better\n",
        "        raise Exception(f\"Failed to generate histogram with `bins`={bins}: {str(e)}\")\n",
        "\n",
        "    if p.sum() == 0:\n",
        "        # If the histogram is empty (all bins are zero)\n",
        "        H = 0\n",
        "    else:\n",
        "        p = p / p.sum()\n",
        "        H = entropy(p)  # Calculate entropy\n",
        "\n",
        "    # Check if normalization of entropy is needed\n",
        "    n_bins = len(bin_edges) - 1  # Number of bins is number of edges minus one\n",
        "    if norm and n_bins > 1: \n",
        "        H /= np.log2(n_bins)\n",
        "    return H\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# calculate_summary_statistics_on_V\n",
        "def calculate_summary_statistics_V(LL):\n",
        "    summary_statistics_V = []\n",
        "    mean_FC = np.zeros(LL.shape[2])\n",
        "    mean_kurtosis = np.zeros(LL.shape[2])\n",
        "    mean_co_kurtosis=np.zeros(LL.shape[2])\n",
        "    mean_max_co_kurtosis=np.zeros(LL.shape[2])\n",
        "    mean_dev_co_kurtosis=np.zeros(LL.shape[2])            \n",
        "    mean_covariance=np.zeros(LL.shape[2])\n",
        "    mean_cross_correlation=np.zeros(LL.shape[2])\n",
        "    var_cross_correlation=np.zeros(LL.shape[2])  \n",
        "    mean_corr_entropy=np.zeros(LL.shape[2]) \n",
        "    var_corr_entropy=np.zeros(LL.shape[2]) \n",
        "        \n",
        "    for cc in range(LL.shape[2]):\n",
        "        data = LL[:, :, cc]\n",
        "        # data has the shape: regions x times\n",
        "        correlation_matrix = np.corrcoef(data)\n",
        "        Kurto = np.mean(np.power(data - np.mean(data, axis=1, keepdims=True), 4), axis=1) / np.power(np.var(data, axis=1), 2)\n",
        "        co_Kurto = np.zeros((LL.shape[0], LL.shape[0]))\n",
        "        corr_entropy = np.zeros((LL.shape[0], LL.shape[0]))\n",
        "\n",
        "        for i in range(LL.shape[0]):\n",
        "            for j in range(LL.shape[0]):\n",
        "                co_Kurto[i,j] = np.mean(np.power(data[i,:] - np.mean(data[i,:], keepdims=True), 2) * np.power(data[j,:] - np.mean(data[j,:], keepdims=True), 2)) / (np.var(data[i,:]) * np.var(data[j,:]))\n",
        "                entropy_i=matrix_entropy(data[i,:], bins='auto', norm=True)\n",
        "                entropy_i.shape\n",
        "                entropy_j=matrix_entropy(data[j,:], bins='auto', norm=True)\n",
        "                corr_entropy[i,j]= np.mean((entropy_i-entropy_j))\n",
        "\n",
        "        mean_FC[cc] = np.mean(correlation_matrix)               \n",
        "        mean_kurtosis[cc] = np.mean(Kurto) \n",
        "        \n",
        "        np.fill_diagonal(co_Kurto, np.nan)\n",
        "        mean_co_kurtosis[cc]=np.nanmean(co_Kurto)\n",
        "\n",
        "        mean_max_co_kurtosis[cc]=np.nanmax(co_Kurto)\n",
        "        mean_dev_co_kurtosis[cc]=np.nanstd(co_Kurto)\n",
        "\n",
        "        mean_covariance[cc] = np.mean(np.cov(data))\n",
        "        var_cross_correlation[cc]=np.var(correlation_matrix) \n",
        "        \n",
        "        mean_corr_entropy[cc]=np.mean(corr_entropy)\n",
        "        var_corr_entropy[cc]=np.var(corr_entropy)\n",
        "\n",
        "        summary_statistics_V.append([\n",
        "            mean_FC, mean_kurtosis,mean_co_kurtosis,mean_max_co_kurtosis, mean_dev_co_kurtosis,\n",
        "            mean_covariance,var_cross_correlation,mean_corr_entropy,var_corr_entropy    \n",
        "        ])\n",
        "        \n",
        "    return np.array(summary_statistics_V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  compute the data features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unique function: calculate_features\n",
        "def calculate_features(LL,threshold):\n",
        "    binarized_matrix = np.where(LL> threshold, 1, 0)\n",
        "    (num_avalan, ttt_noaval)=compute_ATM(binarized_matrix)\n",
        "    summary_statistics=calculate_summary_statistics_on_ATM(ttt_noaval)\n",
        "    summary_statistics_V=calculate_summary_statistics_V(LL)\n",
        "    print(f\"There are {summary_statistics_V.shape[1]} features calculated on signals\")\n",
        "    print(summary_statistics_V.shape)\n",
        "    summary_statistics=np.array(summary_statistics)\n",
        "    print(f\"There are {summary_statistics.shape[1]} features calculated on ATM\")\n",
        "    summary_statistics = np.squeeze(summary_statistics)\n",
        "    summary_statistics_V = summary_statistics_V[0,:,:]\n",
        "    combined_statistics = np.concatenate((summary_statistics, summary_statistics_V), axis=0)\n",
        "    return np.array(combined_statistics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# uploading one simulations composed by 50 different values of ckk_dopa we can have an idea of what happens \n",
        "# for low and high values of ckk_dopa and we can estimate the range of parameters. \n",
        "# Then we also computed the statistics from the data.\n",
        "# the function calculate_features take in input the LL_6 matrix which is the product between the V signals simulated \n",
        "# and the Lead field matrix for each value of ckk_dopa\n",
        "print(f\"The shape of LL_6 is:{LL_6.shape} num_channels x time x number of ckk_dopa values \")\n",
        "threshold=1\n",
        "all_data_features=calculate_features(LL_6,threshold)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"In this way, for each of {all_data_features.shape[1]} c_dopa value, we have {all_data_features.shape[0]} data features\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.save(\"synthetic_data_features.npy\",all_data_features)\n",
        "np.save(\"c_dopa.npy\",matrix_C_XXX[:,2])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
